import os
import sys

from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, CallbackList
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.vec_env.vec_normalize import VecNormalize
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
import torch
import wandb
from wandb.integration.sb3 import WandbCallback

from gymenv import AwesimEnv
# from bindings import *  # noqa
from gymenv_utils import EntropyDecayCallback


experiment_name = "PPO_Lane84_ADAS_3Actions_LayerNorm_Gamma0.999"
notes = """Trains a PPO policy in a gym environment where an agent controls a vehicle using ADAS with 3 actionable parameters, updated every second: cruise speed (±10 mph), turn/safe-merge signal, and follow distance (±1s time-headway). Automatic Emergency Braking is always active with fixed parameters (2 mph, 0.01 mph, 3 ft, 6 ft; see ai.h). The goal is to reach lane #84 (2nd Ave N, between 1st St and S-2) from a random start without crashing, with episodes truncated after 10 minutes. Reward: -0.2/s, +100 for goal, -100 for crash (terminates episode). Observation includes 149 variables: vehicle state/capabilities (varies per episode), ADAS state, map awareness, nearby vehicles, intersection state, road/environment factors. Key hyperparameters: LayerNorm after each hidden layer in a [149, 1024, 512, 256, value/policy] architecture, gamma=0.999."""

config = dict(
    experiment_name=experiment_name,
    n_totalsteps_per_batch=32768,
    ppo_iters=10000,
    normalize=True,
    norm_obs=True,
    norm_reward=True,
)
env_config = dict(
    goal_lane=84,
    city_width=1000,
    num_cars=256,
    decision_interval=1,
    sim_duration=60 * 10
)
vec_env_config = dict(n_envs=64)
ppo_config = dict(
    n_steps=config["n_totalsteps_per_batch"] // vec_env_config["n_envs"],
    learning_rate=0.0001,
    ent_coef=0.0,
    batch_size=256,
    target_kl=0.02,
    gae_lambda=0.95,
    gamma=0.999,
    verbose=1,
    device="cpu",
    tensorboard_log="./logs/awesim_tensorboard/"
)


class MlpWithLayerNorm(BaseFeaturesExtractor):
    def __init__(self, observation_space, features_dim=256, net_arch=None, final_norm_and_act=True):
        super(MlpWithLayerNorm, self).__init__(observation_space, features_dim)
        if net_arch is None:
            net_arch = [features_dim * 4, features_dim * 2]
        n_input = observation_space.shape[0]
        layers = []
        current_dim = n_input
        for hidden_dim in net_arch:
            layers.append(torch.nn.Linear(current_dim, hidden_dim))
            layers.append(torch.nn.LayerNorm(hidden_dim))
            layers.append(torch.nn.ReLU())
            current_dim = hidden_dim
        layers.append(torch.nn.Linear(current_dim, features_dim))
        if final_norm_and_act:
            layers.append(torch.nn.LayerNorm(features_dim))
            layers.append(torch.nn.ReLU())
        self.net = torch.nn.Sequential(*layers)

    def forward(self, observations):
        return self.net(observations)


policy_config = dict(
    net_arch=[],
    features_extractor_class=MlpWithLayerNorm,
    features_extractor_kwargs=dict(net_arch=[1024, 512], features_dim=256),
)
config = dict(**config, **env_config, **vec_env_config, **ppo_config, **policy_config)


def make_env(i=0):
    return AwesimEnv(**env_config, i=i)


if __name__ == "__main__":

    eval_mode = len(sys.argv) > 1

    if not eval_mode:
        vec_env = make_vec_env(make_env, **vec_env_config, vec_env_cls=SubprocVecEnv)
        if config["normalize"]:
            vec_env = VecNormalize(vec_env, norm_obs=config["norm_obs"], norm_reward=config["norm_reward"], training=True, gamma=config["gamma"])  # type: ignore
        model = PPO("MlpPolicy", vec_env, policy_kwargs=policy_config, **ppo_config)
        project = "awesim" if env_config["goal_lane"] is None else "awesim_goal"
        wandb = wandb.init(project=project, name=experiment_name, config=config, notes=notes, sync_tensorboard=True, save_code=True)

        # Save a checkpoint every 500000 steps
        callbacks = CallbackList([
            EntropyDecayCallback(
                start=0.01,
                end=0.001,
                end_fraction=0.9
            ),
            CheckpointCallback(
                save_freq=max(500000 // vec_env_config["n_envs"], 1),
                save_path="./models/" + experiment_name + "/",
                name_prefix=experiment_name,
                save_replay_buffer=False,
                save_vecnormalize=True,
            ),
            WandbCallback(),
        ])

        n_ppo_iters = 10000
        model.learn(total_timesteps=n_ppo_iters * config["n_totalsteps_per_batch"], log_interval=1, callback=callbacks, progress_bar=True, tb_log_name=experiment_name, reset_num_timesteps=True)
        model_path = "models/" + experiment_name + "/model.zip"
        vec_stats_path = "models/" + experiment_name + "/vec_normalize.pkl"
        model.save(model_path)
        if config["normalize"]:
            vec_env.save(vec_stats_path)  # type: ignore
    else:
        load_model_step = int(sys.argv[1])
        model_path = f"models/{experiment_name}/{experiment_name}_{load_model_step}_steps.zip"
        vec_stats_path = f"models/{experiment_name}/{experiment_name}_vecnormalize_{load_model_step}_steps.pkl"
        if not os.path.exists(model_path):
            print(f"Model not found at {model_path}")
            sys.exit(1)
        if config["normalize"] and not os.path.exists(vec_stats_path):
            print(f"Vec normalization stats not found at {vec_stats_path}")
            sys.exit(1)

    env = make_env()
    env.should_render = True
    env.synchronized = True
    env.synchronized_sim_speedup = 16.0
    env.render_server_ip = "127.0.0.1"

    vec_env = make_vec_env(lambda: env)

    print(f"Loading model from {model_path}")
    model = PPO("MlpPolicy", vec_env, policy_kwargs=policy_config, **ppo_config)
    model.set_parameters(model_path)
    print("Model loaded")

    if config["normalize"]:
        print(f"Loading VecNormalize stats from {vec_stats_path}")
        vec_env = VecNormalize.load(vec_stats_path, vec_env)
        vec_env.training = False  # Disable training mode for evaluation
        print("VecNormalize stats loaded")
    obs = vec_env.reset()
    for i in range(10000):
        action, _state = model.predict(obs, deterministic=True)  # type: ignore
        obs, reward, done, info = vec_env.step(action)
        # VecEnv resets automatically when done
